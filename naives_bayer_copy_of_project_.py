# -*- coding: utf-8 -*-
"""Copy of Naives Bayer Copy of PROJECT MORINGASCHOOL.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Epoq4xxr9XjeiJUn9C43PQKSSb03Fhhk
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import string 
from keras.models import Model
from keras.layers import Input, Embedding,SpatialDropout1D, Bidirectional, GRU
from keras.layers import Conv1D, GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, Dense, Dropout
from keras.preprocessing import text, sequence 
from keras.callbacks import EarlyStopping, ModelCheckpoint, Callback
from sklearn.model_selection import train_test_split
import nltk
nltk.download('stopwords')
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report
from sklearn.feature_extraction.text import CountVectorizer
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer 
from nltk.corpus import stopwords, wordnet
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')
nltk.download('omw-1.4')
nltk.download('omw-1.4')

data = pd.read_csv('/content/twitter_sentiment_data.csv')

data

data.info()

data.describe()

data['message']

data=data.drop(['tweetid'],axis=1)
data

sns.heatmap(data.isnull(), yticklabels = False, cbar = False, cmap = 'Blues' )

data.hist(bins=30,figsize=(13,5),color='r')

data['length'] = data['message'].apply(len)
data

data['length'].plot(bins = 100, kind='hist')

data.describe()

"""Shortest tweet"""

data[data['length']==7]['message'].iloc[0]

neutral = data[data['sentiment']==0]
neutral

pro = data[data['sentiment']==1]
pro

anti = data[data['sentiment']==-1]
anti

news = data[data['sentiment']==2]
news

sentences = data['message'].tolist()
sentences

len(sentences)

sentences_as_one_string = " ".join(sentences)
sentences_as_one_string

!pip install WordCloud
from wordcloud import WordCloud

plt.figure(figsize=(20,20))
plt.imshow(WordCloud().generate(sentences_as_one_string))

data

"""Removing punctuation and  stopwords


"""

string.punctuation

def message_cleaning(message):
  test_punc_removed = [char for char in message if char not in string.punctuation]
  test_punc_removed_join = ''.join(test_punc_removed)
  test_punc_removed_join_clean = [word for word in test_punc_removed_join.split() if word.lower() not in stopwords.words('english') ]
  return test_punc_removed_join_clean

data_clean = data['message'].apply(message_cleaning)
print(data_clean[4])

print(data['message'][4])

vectorizer = CountVectorizer(analyzer = message_cleaning)
tweets_countvectorizer = CountVectorizer(analyzer = message_cleaning, dtype = 'uint8').fit_transform(data['message']).toarray()

np.shape(tweets_countvectorizer)

x = tweets_countvectorizer

y = data['sentiment']

x.shape

y.shape

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2)

from sklearn.naive_bayes import MultinomialNB
NB_classifier = MultinomialNB()
NB_classifier.fit(x_train, y_train)

y_predict_test = NB_classifier.predict(x_test)
cm = confusion_matrix(y_test, y_predict_test)
sns.heatmap(cm, annot=True)

print(classification_report(y_test, y_predict_test))